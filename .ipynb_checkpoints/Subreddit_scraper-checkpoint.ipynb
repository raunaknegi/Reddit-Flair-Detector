{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import isfile\n",
    "import praw\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "\n",
    "reddit = praw.Reddit(client_id='', \\\n",
    "                     client_secret='', \\\n",
    "                     user_agent='', \\\n",
    "                     username='', \\\n",
    "                     password='')\n",
    "\n",
    "class SubredditScraper:\n",
    "\n",
    "    def __init__(self, sub,time_=' all', sort='new', lim=900, mode='w'):\n",
    "        self.sub = sub\n",
    "        self.time_ = time_\n",
    "        self.sort = sort\n",
    "        self.lim = lim\n",
    "        self.mode = mode        \n",
    "\n",
    "        print(\n",
    "            f'SubredditScraper instance created with values '\n",
    "            f'sub = {sub}, sort = {sort}, lim = {lim}, mode = {mode}')\n",
    "        print(time_)\n",
    "        print(sort)\n",
    "\n",
    "    def set_sort(self):\n",
    "        if self.sort == 'new':\n",
    "            return self.sort, reddit.subreddit(self.sub).new(limit=self.lim)\n",
    "        elif self.sort == 'top':\n",
    "            return self.sort, reddit.subreddit(self.sub).top(limit=self.lim,time_filter=self.time_)\n",
    "        elif self.sort == 'hot':\n",
    "            return self.sort, reddit.subreddit(self.sub).hot(limit=self.lim)\n",
    "        elif self.sort=='rising':\n",
    "            return self.sort, reddit.subreddit(self.sub).rising(limit=self.lim)\n",
    "        elif self.sort=='controversial':\n",
    "            return self.sort,reddit.subreddit(self.sub).controversial(limit=self.lim,time_filter=self.time_)            \n",
    "        else:\n",
    "            self.sort = 'hot'\n",
    "            print('Sort method was not recognized, defaulting to hot.')\n",
    "            return self.sort, reddit.subreddit(self.sub).hot(limit=self.lim)\n",
    "\n",
    "    def get_submissions(self):\n",
    "        \"\"\"Get unique posts from a specified subreddit.\"\"\"\n",
    "\n",
    "        sub_dict = {\n",
    "            'id': [],'body': [], 'title': [],'flair':[],'comment':[]}\n",
    "        csv = f'{self.sub}_posts.csv'\n",
    "\n",
    "        sort, subreddit = self.set_sort()\n",
    "\n",
    "        df, csv_loaded = (pd.read_csv(csv), 1) if isfile(csv) else ('', 0)\n",
    "\n",
    "        print(f'csv = {csv}')\n",
    "        print(f'After set_sort(), sort = {sort} and sub = {self.sub}')\n",
    "        print(f'csv_loaded = {csv_loaded}')\n",
    "\n",
    "        print(f'Collecting information from r/{self.sub}.')\n",
    "        for submission in subreddit:\n",
    "\n",
    "            # Check if post.id is in df and set to True if df is empty.\n",
    "            # This way new posts are still added to dictionary when df = ''\n",
    "            unique_id = submission.id not in tuple(df.id) if csv_loaded else True\n",
    "\n",
    "            # Save any unique, non-stickied posts with descriptions to sub_dict.\n",
    "            if unique_id:\n",
    "                sub_dict['id'].append(submission.id)\n",
    "                sub_dict['body'].append(submission.selftext)\n",
    "                sub_dict['title'].append(submission.title)               \n",
    "                sub_dict['flair'].append(submission.link_flair_text)\n",
    "            \n",
    "                submission.comments.replace_more(limit=None)\n",
    "                comment = ''\n",
    "                count = 0\n",
    "                for top_level_comment in submission.comments:\n",
    "                    comment = comment + ' ' + top_level_comment.body\n",
    "                    count+=1     \n",
    "                    if(count > 10):\n",
    "                        break\n",
    "\n",
    "                sub_dict[\"comment\"].append(str(comment))\n",
    "\n",
    "        # pprint(sub_dict)\n",
    "        new_df = pd.DataFrame(sub_dict)\n",
    "\n",
    "        # Add new_df to df if df exists then save it to a csv.\n",
    "        if 'DataFrame' in str(type(df)) and self.mode == 'w':\n",
    "            pd.concat([df, new_df], axis=0, sort=0).to_csv(csv, index=False)\n",
    "            print(\n",
    "                f'{len(new_df)} new posts collected and added to {csv}')\n",
    "        elif self.mode == 'w':\n",
    "            new_df.to_csv(csv, index=False)\n",
    "            print(f'{len(new_df)} posts collected and saved to {csv}')\n",
    "        else:\n",
    "            print(\n",
    "                f'{len(new_df)} posts were collected but they were not '\n",
    "                f'added to {csv} because mode was set to \"{self.mode}\"')\n",
    "            \n",
    "    def get_top(self):\n",
    "        sub_dict = {\n",
    "        'id': [], 'body': [], 'title': [], 'flair':[],'comments':[]}\n",
    "        csv = f'new_{self.sub}_posts.csv'\n",
    "        subreddit=reddit.subreddit(self.sub).top(limit=self.lim,time_filter='all')\n",
    "        df, csv_loaded = (pd.read_csv(csv), 1) if isfile(csv) else ('', 0)\n",
    "\n",
    "        print(f'csv = {csv}')\n",
    "        print(f'csv_loaded = {csv_loaded}')\n",
    "\n",
    "        print(f'Collecting information from r/{self.sub}.')\n",
    "        for submission in subreddit:\n",
    "\n",
    "            # Check if post.id is in df and set to True if df is empty.\n",
    "            # This way new posts are still added to dictionary when df = ''\n",
    "            unique_id = submission.id not in tuple(df.id) if csv_loaded else True\n",
    "\n",
    "            # Save any unique, non-stickied posts with descriptions to sub_dict.\n",
    "            if unique_id:\n",
    "                sub_dict['id'].append(submission.id)\n",
    "                sub_dict['body'].append(submission.selftext)\n",
    "                sub_dict['title'].append(submission.title)\n",
    "                sub_dict['flair'].append(submission.link_flair_text)\n",
    "            sleep(0.1)\n",
    "\n",
    "        # pprint(sub_dict)\n",
    "        new_df = pd.DataFrame(sub_dict)\n",
    "\n",
    "        # Add new_df to df if df exists then save it to a csv.\n",
    "        if 'DataFrame' in str(type(df)) and self.mode == 'w':\n",
    "            pd.concat([df, new_df], axis=0, sort=0).to_csv(csv, index=False)\n",
    "            print(\n",
    "                f'{len(new_df)} new posts collected and added to {csv}')\n",
    "        elif self.mode == 'w':\n",
    "            new_df.to_csv(csv, index=False)\n",
    "            print(f'{len(new_df)} posts collected and saved to {csv}')\n",
    "        else:\n",
    "            print(\n",
    "                f'{len(new_df)} posts were collected but they were not '\n",
    "                f'added to {csv} because mode was set to \"{self.mode}\"')\n",
    "        \n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     SubredditScraper('India', lim=997, mode='w', sort='top').get_submissions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubredditScraper instance created with values sub = India, sort = new, lim = 2000, mode = w\n",
      "yearly\n",
      "new\n",
      "csv = India_posts.csv\n",
      "After set_sort(), sort = new and sub = India\n",
      "csv_loaded = 0\n",
      "Collecting information from r/India.\n",
      "408 posts collected and saved to India_posts.csv\n"
     ]
    }
   ],
   "source": [
    "SubredditScraper('India',lim=2000,mode='w',sort='new',time_='yearly').get_submissions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubredditScraper instance created with values sub = India, sort = top, lim = 2000, mode = w\n",
      "year\n",
      "top\n",
      "csv = India_posts.csv\n",
      "After set_sort(), sort = top and sub = India\n",
      "csv_loaded = 1\n",
      "Collecting information from r/India.\n",
      "998 new posts collected and added to India_posts.csv\n"
     ]
    }
   ],
   "source": [
    "SubredditScraper('India',lim=2000,mode='w',sort='top',time_='year').get_submissions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubredditScraper instance created with values sub = India, sort = hot, lim = 2000, mode = w\n",
      "month\n",
      "hot\n",
      "csv = India_posts.csv\n",
      "After set_sort(), sort = hot and sub = India\n",
      "csv_loaded = 1\n",
      "Collecting information from r/India.\n",
      "4 new posts collected and added to India_posts.csv\n"
     ]
    }
   ],
   "source": [
    "SubredditScraper('India',lim=2000,mode='w',sort='hot',time_='month').get_submissions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
